\documentclass[10pt]{article}

\title{Data Science 210 Final Project}
\author{Thomas Kwashnak}
\date{Fall 2021}
\usepackage{amsmath}
\usepackage{soul}
\usepackage{algorithm2e}
% \usepackage[citestyle=alphabetic,bibstyle=authortitle]{biblatex}
\usepackage{listings}
\usepackage{enumitem}
\usepackage[margin=1in]{geometry}


\begin{document}
\maketitle
\setlength{\parindent}{0pt}.
\setlength{\parskip}{\baselineskip}
\lstset{numbers=left, numberstyle=\footnotesize, frame=l} 
\RestyleAlgo{ruled}


% These sections are good for a start, however change them up as needed
\section{Background}
% State Algorithm chosen and state what task we'll compete
% Brief outline on mathmatics behind algorithm
% Summary of Dataset
\section{Design}
During this section, a variety of variables and variable syntax will be used. The following is a list that explains each variable. Remember that for a given "layer", the output is considered the 0th layer, the hidden layer is considered the 1st layer, and the input is considered the 2nd layer.
\begin{description}[style=nextline]
    \item[$\vec{y}^n$] The output values of the nodes in the $n^{\text{th}}$ hidden layer. 
    \item[$y^n_i$] The output value of node $i$ in the $n^{\text{th}}$ hidden layer.
    \item[$y$] By default, if y is left alone, it represents the output value, known as $y^0_0$ 
    \item[$\hat{y}$] The expected final output 
    \item[$W^n$] The weight matrix for a given layer $n$, as it applies to the values $\vec{y}^{n+1}$ (the outputs of the previous layer) 
    \item[$w^n_{i,j}$] The weight value for the value passed from node $j$ of layer $n+1$ as it is passed to node $i$ of layer $n$
\end{description}
\subsection{Derivations and Equations}
The first part of designing the algorithms needed is to derive the math that will be needed in order to do every necessary function. Each of the following sub-sections describe how a given equation is derived.

\subsubsection{The $\sigma$ function}
The sigmoid activation function that will be used for all nodes (apart from the input layer, which uses a linear activation function) can be explained as follows:
$$\sigma(n) = \frac{1}{1 + e^{-n}}$$
$$\sigma'(n) = \sigma(n) * (1 - \sigma(n))$$
When using this function on a matrix or vector, we simply just apply the sigma function for all individual values in the matrix/vector. 

\subsubsection{Forward Feeding Equation}
This equation represents what the output will be for a given input $\vec{y^2}$. The basic principal is derived from the following relationship for the ouputs of layer $n$:
$$\vec{y^n} = \sigma(W_{n} * \vec{y^{n+1}})$$
We can chain this rule to find the output vector. Since the output vector is a 1x1 matrix, it will result in a scalar value
$$y = \vec{y^0} = \sigma(W_0 * \sigma(W_1 * \vec{y^2}))$$
% % OK SO MAYBE WE SHOULD LIKE.. NOT HAVE THIS
% We can then simplify the first layer by writing the manual multiplication:
% $$y = \sigma(\sum_i{(w^0_{0,i} * \sigma(W^1 * \vec{y^2})_{i,0})})$$
\subsubsection{The Loss Function}
The loss function is the overall function we wish to minimize, and is as follows:
$$L = (y^0_0 - \hat{y})^2 = (y - \hat{y})^2$$

\subsubsection{Derivative of Layer-0 Weights}
In back propagation, we need to find the derivative of each of the weights in terms of the loss function. First, we need to find the derivative of the loss function with respect to the weights that connect the hidden layer with the output layer. The derivative can be simplified using the chain rule as follows:
$$\frac{\delta L}{\delta w^0_{0,i}} = \frac{\delta L}{\delta y} * \frac{\delta y}{\delta w^0_{0,i}}$$
We can factor this out to be the following. Note that $\sigma(\ldots) = \sigma(\sum_i{(w^0_{0,i} * \sigma(W^1 * \vec{y^2})_{i,0})}) = y$.
$$\frac{\delta L}{\delta w^0_{0,i}} = 2(y - \hat{y}) * (\sigma'(\ldots)) * (\sigma((W^1 * \vec{y^2})_{i,0}))$$
$$\frac{\delta L}{\delta w^0_{0,i}} = 2(y - \hat{y}) * (\sigma(\ldots)(1 - \sigma(\ldots))) * (\sigma(\sum_h{(w^1_{i,h}* y^2_{h})})$$
$$\frac{\delta L}{\delta w^0_{0,i}} = 2(y - \hat{y}) * (y * (1 - y)) * (\sigma(\sum_h{(w^1_{i,h} y^2_{h})})$$
$$\frac{\delta L}{\delta w^0_{0,i}} = 2(y - \hat{y}) * (y - y^2) *  (\sigma(\sum_h^{}{(w^1_{i,h} *y^2_{h})})$$
And now, the true factored algorithm is:
$$\frac{\delta L}{\delta w^0_{0,i}} = \frac{2(y - \hat{y})(y - y^2)}{1 + e^{-(\sum_h^{}{(w^1_{i,h} *y^2_{h})})}}$$

\subsubsection{Derivative of Layer-1 Weights}
Next, we need to find the derivative of each of the weights that connect the input layer to the hidden layer. We can write this derivative as follows:
$$\frac{\delta L}{\delta w^1_{i,j}} = \frac{\delta L}{\delta w^0_{0,i}} * \frac{\delta w^0_{0,i}}{\delta y^1_i} * \frac{\delta y^1_i}{\delta w^1_{i,j}}$$
Thus, with using $\frac{\delta L}{\delta w^0_{0,i}}$, we can derive $\frac{\delta L}{\delta w^1_{i,j}}$. We will keep $\frac{\delta L}{\delta w^0_{0,i}}$ in as itself, since in our algorithms we will be able to store this value from the previous step. In this instance, $\sigma(\ldots) = \sigma(W^1 * \vec{y^2}) = y^1_i$.
$$\frac{\delta L}{\delta w^1_{i,j}} = \frac{\delta L}{\delta w^0_{0,i}} * (2(y-\hat{y})(y - y^2)(\sigma'(\ldots))) * (w^1_{i,j} * y^2_{j})$$
$$\frac{\delta L}{\delta w^1_{i,j}} = \frac{\delta L}{\delta w^0_{0,i} }* (2(y - \hat{y})(y - y^2)(\sigma(\ldots)(1 - \sigma(\ldots)))) * (w^1_{i,j} * y^2_{j})$$
$$\frac{\delta L}{\delta w^1_{i,j}} = \frac{\delta L}{\delta w^0_{0,i}} * (2(y - \hat{y})(y - y^2)(y^1_i (1 - y^1_i))) * (w^1_{i,j} * y^2_{j})$$
$$\frac{\delta L}{\delta w^1_{i,j}} = \frac{\delta L}{\delta w^0_{0,i}} * (2(y - \hat{y})(y - y^2)((y^1_i) - (y^1_i)^2) * (w^1_{i,j} * y^2_{j})$$


% Pseudocode, referenced previous algorithms (just include their pseudocode)
\section{Implementation}
\subsection{Data Importing}
Since the data was given in a string csv format, an additional python script was created to import the data into two matricies, one contianing the inputs and one contianing the expected outputs.
% Reference methods / describe code?
\section{Validation}
\section{Reflection}


\end{document}