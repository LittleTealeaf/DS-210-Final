\documentclass[10pt]{article}

\title{Data Science 210 Final Project}
\author{Thomas Kwashnak}
\date{Fall 2021}
\usepackage{amsmath}
\usepackage{soul}
\usepackage[ruled,linesnumbered]{algorithm2e}
\usepackage{listings}
\usepackage{enumitem}
\usepackage[margin=1in]{geometry}
\usepackage{float}


\begin{document}
\maketitle
\setlength{\parindent}{0pt}.
\setlength{\parskip}{\baselineskip}
\lstset{numbers=left, numberstyle=\footnotesize, frame=l} 
\RestyleAlgo{ruled}

\tableofcontents
\newpage

% These sections are good for a start, however change them up as needed
\section{Background}
% State Algorithm chosen and state what task we'll compete
% Brief outline on mathmatics behind algorithm
% Summary of Dataset
\section{Design}
During this section, a variety of variables and variable syntax will be used. The following is a list that explains each variable. Remember that for a given "layer", the output is considered the 0th layer, the hidden layer is considered the 1st layer, and the input is considered the 2nd layer.
\begin{description}[style=nextline]
    \item[$\vec{y}^n$] The output values of the nodes in the $n^{\text{th}}$ hidden layer. 
    \item[$y^n_i$] The output value of node $i$ in the $n^{\text{th}}$ hidden layer.
    \item[$y$] By default, if y is left alone, it represents the output value, known as $y^0_0$ 
    \item[$\hat{y}$] The expected final output 
    \item[$W^n$] The weight matrix for a given layer $n$, as it applies to the values $\vec{y}^{n+1}$ (the outputs of the previous layer) 
    \item[$w^n_{i,j}$] The weight value for the value passed from node $j$ of layer $n+1$ as it is passed to node $i$ of layer $n$
\end{description}

I've sectioned off the design of the algorithms into the independant algorithms, ordering them to try and explain the progression of deriving the algorithms. 

\subsection{Sigmoid Activation Function}
In my neural network, we will use an activation function to condense the output of a node down to a [0-1] scale. In this lab, we will use a sigmoid function. Below is the sigmoid function and it's derivative.
$$\sigma(n) = \frac{1}{1 + e^{-n}}$$
$$\sigma'(n) = \sigma(n) \cdot (1 - \sigma(n))$$

When using this function on a matrix or vector, we simply just apply the sigma function for all individual values in the matrix/vector. The pseudocode algorithms of these functions are as follows:

\begin{algorithm}[H]
    \caption{$\sigma(n)$ function for both constants and matrices}
    \KwIn{$A$ - which can either be a constant, or a $n$ x $m$ matrix}
    \KwOut{$B$ - which is equivilant to $\sigma(A)$}
    \uIf{$A$ is a Matrix}{
        $B \gets $new Matrix($n$ x $m$) of 0s\\
        \For{$i = 0, 1, 2... n$}{
            \For{$j = 0, 1, 2... m$}{
                $B[i][j] \gets \sigma(A[i,j])$
            }
        }
        \Return{$B$}
    }
    \Else{
        \Return{$1 / (1 + e^{-A})$}
    }
\end{algorithm}

\subsection{Forward Feeding Algorithm}
The forward feeding algorithm is what we use in order to calculate the resulting number from a given input into the neural network. We are given an input layer: $\vec{y^2}$, and the following weight matrices $W^0$, $W^1$. Mathematically, the forward feeding algorithm is a recursive algorithm of the following structure:
$$\vec{y^n} = \sigma(W_{n} \cdot \vec{y^{n+1}})$$
Since we have two layers and an output layer, our function for the whole network is described as follows:
$$y = y^0_{0,0} = \sigma(W_0 \cdot \sigma(W_1 \cdot \vec{y^2}))_{0,0}$$
\subsection{Loss Function}
In order to measure the effectiveness of the function, we need to use a loss function that relates the output of the neural network to the expected result (from the data). We will use a simple loss function that takes the difference of these values, and then squares it to remove any negatives. We can use backwards propagation in order to change values to minimize this error, training the network to get our desired result.
$$L = (y^0_0 - \hat{y})^2 = (y - \hat{y})^2$$

\subsection{$0^{\text{th}}$ Layer Weight Derivative}
In back propagation, we need to find the derivative of each of the weights in terms of the loss function. First, we need to find the derivative of the loss function with respect to the weights that connect the hidden layer with the output layer.\\
Given the following variables:

$$y^1_{i,0} = \sigma(\sum_h{w^1_{i,h} \cdot y^2_{h,0}})$$
$$y^0_{0,0} = \sigma(\sum_h{w^0_{0,h} \cdot y^1_{h,0}})$$
We can calculate the derivative as follows:
$$\frac{\delta L}{\delta w^0_{0,i}} = \frac{\delta L}{\delta y^0_{0,0}} \cdot \frac{\delta y^0_{0,0}}{\delta \sum_h{w^0_{0,h} \cdot y^1_{h,0}}} \cdot \frac{\delta \sum_h{w^0_{0,h} \cdot y^1_{h,0}}}{\delta w^0_{0,i}}$$
$$\frac{\delta L}{\delta w^0_{0,i}} = 2 \cdot (y^0_{0,0} - \hat{y}) \cdot \sigma'(\sum_h{w^0_{0,h} \cdot y^1_{h,0}}) \cdot y^1_{i,0}$$
$$\frac{\delta L}{\delta w^0_{0,i}} = 2 \cdot (y^0_{0,0} - \hat{y}) \cdot \sigma(\sum_h{w^0_{0,h} \cdot y^1_{h,0}}) \cdot (1 - \sigma(\sum_h{w^0_{0,h} \cdot y^1_{h,0}})) \cdot y^1_{i,0}$$
$$\frac{\delta L}{\delta w^0_{0,i}} = 2\cdot (y^0_{0,0} - \hat{y}) \cdot y^0_{0,0} \cdot (1 - y^0_{0,0}) \cdot y^1_{i,0}$$

\subsection{$1^{\text{st}}$ Layer Weight Derivative}
Next, we need to find the derivative of each of the weights that connect the input layer to the hidden layer. Given the following variables:
$$y^1_{i,0} = \sigma(\sum_h{w^1_{i,h} \cdot y^2_{h,0}})$$
$$y^0_{0,0} = \sigma(\sum_h{w^0_{0,h} \cdot y^1_{h,0}})$$
We can calculate the derivative as the following:
$$\frac{\delta L}{\delta w^1_{i,j}} = \frac{\delta L}{\delta y^0_{0,0}} \cdot \frac{\delta y^0_{0,0}}{\delta \sum_h{w^0_{0,h} \cdot y^1_{h,0}}} \cdot \frac{\delta \sum_h{w^0_{0,h} \cdot y^1_{h,0}}}{\delta y^1_{i,0}} \cdot \frac{\delta y^1_{i,0}}{\delta \sum_h{w^1_{i,h} \cdot y^2_h}} \cdot \frac{\delta \sum_h{w^1_{i,h} \cdot y^2_{h,0}}}{\delta w^1_{i,j}}$$
$$\frac{\delta L}{\delta w^1_{i,j}} = 2\cdot (y^0_{0,0} - \hat{y}) \cdot \sigma'(\sum_h{w^0_{0,h} \cdot y^1_{h,0}}) \cdot w^0_{0,i} \cdot \sigma'(\sum_h{w^1_{i,h} \cdot y^2_{h,0}}) \cdot y^2_{j,0}$$
$$\frac{\delta L}{\delta w^1_{i,j}} = 2 \cdot (y^0_{0,0} - \hat{y}) \cdot \sigma(\sum_h{w^0_{0,h} \cdot y^1_{h,0}}) \cdot (1 - \sigma(\sum_h{w^0_{0,h} \cdot y^1_{h,0}})) \cdot w^0_{0,i} \cdot \sigma(\sum_h{w^1_{i,h} \cdot y^2_{h,0}}) \cdot (1 - \sigma(\sum_h{w^1_{i,h} \cdot y^2_{h,0}})) \cdot  y^2_{j,0}$$
$$\frac{\delta L}{\delta w^1_{i,j}} = 2 \cdot (y^0_{0,0} - \hat{y}) \cdot y^0_{0,0} \cdot (1 - y^0_{0,0}) \cdot w^0_{0,i} \cdot y^1_{i,0} \cdot (1 - y^1_{i,0}) \cdot y^2_{j,0}$$
We can then reorder it as..
$$\frac{\delta L}{\delta w^1_{i,j}} = 2 \cdot (y^0_{0,0} - \hat{y}) \cdot y^0_{0,0} \cdot (1 - y^0_{0,0})\cdot y^1_{i,0} \cdot w^0_{0,i}  \cdot (1 - y^1_{i,0}) \cdot y^2_{j,0}$$
And substitute in the $0^{\text{th}}$ layer derivative!
$$\frac{\delta L}{\delta w^1_{i,j}} = \frac{\delta L}{\delta w^0_{0,i}} \cdot w^0_{0,i}  \cdot (1 - y^1_{i,0}) \cdot y^2_{j,0} $$

\subsection{Backwards Propagation}
This is the heart of the whole lab project, backwards propagation. The goal of backwards propagation is to modify the values of the weights used in the neural network in order to "train" the network on a given output. Since we are trying to minimize the error term, we want to nudge the inputs in such a way that the output gets closer to the value we want. The algorithm below will iterate through a given. This algorithm is specifically designed for this given context, meaning it has been hard-coded for the 3 layer network that we have.\newline
\begin{algorithm}[H]
    \caption{Backwards Propagation for a 2-layer neural network}
    \KwIn{
        $Y^2$ - The input vector (matrix), dimensions $m$ x $1$.\newline
        $W^1_\text{in}$ - The weight matrix, dimensions $h$ x $m$, that represents the weights used as variables go from $Y^2 \rightarrow Y^0$\newline
        $W^0_\text{in}$ - The weight matrix, dimensions $1$ x $h$, that represents the weights used as variables go from $Y^1 \rightarrow Y^0$\newline
        $S$ - The step coefficient to modify weights by, as a scalar value.\newline
        $\hat{y}$ - The expected result from Forward-Feeding
    }
    \KwOut{
        $W^1$ - The modified weight matrix, describing the same thing as $W^1_\text{in}$\newline
        $W^0$ - The modified weight matrix, describing the same thing as $W^0_\text{in}$
    }
    $W^1 \gets \text{copy of } W^1_\text{in}$\\
    $W^0 \gets \text{copy of } W^0_\text{in}$\\
    $Y^1 \gets \sigma(W^1 \cdot Y^2)$\\
    $Y^0 \gets \sigma(W^0 \cdot Y^1)$\\
    \For{$i \gets (0, 1, \ldots, h-1)$}{
        $\Delta W^0 \gets S \cdot 2 \cdot (Y^0_{0,0} - \hat{y}) \cdot Y^0_{0,0} \cdot (1 - Y^0_{0,0}) \cdot Y^1_{i,0}$\\
        $W^0_{0,i} \gets W^0_{0,i} + \Delta W^0$\\
        \For{$j \gets (0, 1, \ldots, m-1)$}{
            $W^1_{i,j} \gets W^1_{i,j} + \Delta W^0 \cdot W^0_{0,i}  \cdot (1 - y^1_{i,0}) \cdot y^2_{j,0}$
        }
    }
    \Return{$W^1, W^0$}
\end{algorithm}

% Pseudocode, referenced previous algorithms (just include their pseudocode)
\section{Implementation}
\subsection{Data Importing}
Since the data was given in a string csv format, an additional python script was created to import the data into two matricies, one contianing the inputs and one contianing the expected outputs.
% Reference methods / describe code?
\section{Validation}
\section{Reflection}
\end{document}